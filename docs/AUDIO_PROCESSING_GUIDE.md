# Audio Processing Guide - Whisper & Sound Detection

## üé§ Overview

Overwatch now supports **audio processing** alongside video detection! Extract audio from RTSP streams and process with AI for:
- üéôÔ∏è **Speech Transcription** (Whisper)
- üîä **Sound Classification** (Gunshots, alarms, glass breaking, etc.)
- üö® **Security Alerts** (Keyword detection in speech)
- üìù **Meeting Transcription**
- üè≠ **Industrial Monitoring** (Machine sounds, anomalies)

---

## üéµ Audio Nodes

### 1. **Audio Extractor Node**
Extracts audio stream from video sources (RTSP, cameras, YouTube).

**Configuration:**
- **Sample Rate**: 8kHz - 48kHz (16kHz recommended for speech)
- **Channels**: Mono (1) or Stereo (2)
- **Format**: WAV, MP3, FLAC, PCM
- **Buffer Duration**: 1-60 seconds (how often to process)

**Use Cases:**
- Extract audio for Whisper transcription
- Feed to sound classifiers
- Audio-only monitoring
- Acoustic analysis

### 2. **Audio AI Node**
Process audio with AI models for transcription or sound classification.

**Two Modes:**

#### A. **Transcription (Whisper)**
Convert speech to text in 99 languages.

**Models:**
- **Whisper Tiny** - Fastest (39M params, real-time capable)
- **Whisper Base** - Balanced (74M params, general use)
- **Whisper Small** - High quality (244M params, professional)
- **Whisper Medium** - Very high quality (769M params)
- **Whisper Large** - Best quality (1550M params, forensics)

**Features:**
- Auto language detection
- 99+ language support
- Keyword trigger alerts
- Timestamp generation
- Speaker diarization (future)

#### B. **Sound Classification**
Detect and classify environmental sounds.

**Models:**
- **YAMNet** - Fast (521 sound classes)
- **AST** - Transformer-based (527 classes)
- **PANNs CNN14** - Neural network (527 classes)

**Detectable Sounds:**
- üî´ Gunshots
- üí• Explosions
- üö® Alarms (fire, burglar)
- üî® Glass breaking
- üò± Screaming
- üêï Dog barking
- üöó Vehicle engines
- üîä Loud noises
- üì¢ Speech/conversation
- üéµ Music
- And 500+ more!

---

## üîß Workflow Examples

### Example 1: Speech Transcription with Keywords

**Scenario**: Transcribe audio and alert on security keywords

```
Camera (RTSP) ‚Üí Audio Extractor ‚Üí Whisper (Base) ‚Üí Action (Alert)
                                        ‚Üì
                                   Debug Console

Settings:
- Sample Rate: 16000 Hz (optimal for speech)
- Language: Auto Detect
- Keywords: ["help", "emergency", "gun", "fire"]
- Buffer: 5 seconds
```

**Output:**
```json
{
  "text": "Help! Someone call security!",
  "language": "en",
  "confidence": 0.95,
  "keywords_detected": ["help", "security"],
  "timestamp": "2025-10-30T23:15:42Z"
}
```

### Example 2: Gunshot Detection

**Scenario**: Detect gunshots and glass breaking for security

```
Camera (Parking) ‚Üí Audio Extractor ‚Üí YAMNet Classifier ‚Üí Action (Emergency Alert)
                                           ‚Üì
                                      Debug Console

Settings:
- Sample Rate: 44100 Hz (high quality)
- Confidence: 0.85 (reduce false positives)
- Target Sounds: Gunshots, Glass breaking, Explosions
- Buffer: 2 seconds
```

**Output:**
```json
{
  "sound": "gunshot",
  "confidence": 0.92,
  "timestamp": "2025-10-30T23:15:42Z",
  "location": "Parking Lot Camera 1"
}
```

### Example 3: Multi-Modal Detection

**Scenario**: Detect person AND listen for keywords

```
              ‚îå‚Üí YOLOv8 (Person Detection) ‚îÄ‚îê
Camera ‚Üí Split                               ‚îú‚Üí Combine ‚Üí Alert
              ‚îî‚Üí Audio Extractor ‚Üí Whisper ‚îÄ‚îÄ‚îò

Alert Triggers:
- Person detected AND
- Keywords: ["help", "emergency"] spoken
```

### Example 4: 24/7 Transcription Archive

**Scenario**: Transcribe all audio for searchable archive

```
Camera ‚Üí Audio Extractor ‚Üí Whisper (Tiny) ‚Üí Database Storage
                                          ‚Üì
                                    Text Search Index

Settings:
- Buffer: 30 seconds
- Language: Auto
- Continuous: Yes
- Store: PostgreSQL with full-text search
```

---

## üéØ Use Cases

### Security Applications

**1. Gunshot Detection**
```
Model: YAMNet
Sounds: Gunshot, Explosion
Confidence: 0.85
Action: Immediate alert + recording
```

**2. Glass Breaking Alert**
```
Model: PANNs CNN14
Sounds: Glass breaking, Smashing
Confidence: 0.80
Action: Security notification
```

**3. Distress Call Detection**
```
Model: Whisper Base
Keywords: ["help", "emergency", "stop", "police"]
Action: Emergency alert with audio clip
```

### Business Applications

**4. Meeting Transcription**
```
Model: Whisper Small
Language: Auto
Output: Full transcript with timestamps
Storage: Document database
```

**5. Customer Service Monitoring**
```
Model: Whisper Base
Keywords: ["complaint", "manager", "refund"]
Action: Flag for review
```

**6. Drive-Through Audio**
```
Model: Whisper Tiny (real-time)
Use: Order taking + verification
Output: Order text + confidence
```

### Industrial Applications

**7. Machine Anomaly Detection**
```
Model: YAMNet
Baseline: Normal machine sounds
Alert: Unusual sounds (bearing failure, etc.)
```

**8. Safety Compliance**
```
Model: YAMNet
Detect: Alarm sounds, sirens
Action: Verify evacuation compliance
```

---

## ‚öôÔ∏è Configuration Guide

### Choosing Sample Rate

| Sample Rate | Best For | Quality |
|-------------|----------|---------|
| 8kHz | Voice-only, low bandwidth | Phone quality |
| 16kHz | Speech recognition (Whisper) | Good speech |
| 22kHz | General audio | CD-like |
| 44.1kHz | High quality sounds | CD quality |
| 48kHz | Professional audio | Studio quality |

**Recommendation**: 
- Speech (Whisper): 16kHz
- Sound Detection: 44.1kHz
- Real-time: 16kHz or lower

### Choosing Buffer Duration

| Duration | Best For | Latency |
|----------|----------|---------|
| 1-2s | Real-time alerts | Very low |
| 5s | Balanced detection | Low |
| 10-15s | Speech segments | Medium |
| 30s+ | Transcription archive | High |

**Recommendation**:
- Gunshot detection: 2s
- Keyword alerts: 5s
- Meeting transcription: 30s

### Choosing Whisper Model

| Model | Speed | Accuracy | Use Case |
|-------|-------|----------|----------|
| Tiny | ‚ö°‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Real-time, edge devices |
| Base | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | General transcription |
| Small | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Professional use |
| Medium | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Legal/medical |
| Large | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Forensics/research |

---

## üõ†Ô∏è Backend Implementation

### Audio Processing Pipeline

```python
1. Extract audio from RTSP stream (FFmpeg)
2. Convert to target sample rate
3. Buffer audio chunks (5s segments)
4. Send to AI model:
   - Whisper ‚Üí Text transcription
   - YAMNet ‚Üí Sound classification
5. Post-process results
6. Trigger actions if keywords/sounds detected
```

### Example Backend Code

```python
# Audio extraction
import ffmpeg
import whisper
import numpy as np

def extract_audio_chunk(rtsp_url, duration=5, sample_rate=16000):
    """Extract audio chunk from RTSP stream"""
    process = (
        ffmpeg
        .input(rtsp_url, rtsp_transport='tcp')
        .audio
        .output('pipe:', format='f32le', acodec='pcm_f32le', 
                ac=1, ar=sample_rate, t=duration)
        .run_async(pipe_stdout=True, pipe_stderr=True)
    )
    
    audio_data = process.stdout.read(sample_rate * duration * 4)
    audio_array = np.frombuffer(audio_data, dtype=np.float32)
    
    return audio_array

# Whisper transcription
model = whisper.load_model("base")
audio = extract_audio_chunk(rtsp_url)
result = model.transcribe(audio, language='en')

print(result['text'])
# Output: "Hello, this is a test recording"

# Keyword detection
keywords = ['help', 'emergency', 'fire']
detected = [kw for kw in keywords if kw.lower() in result['text'].lower()]

if detected:
    trigger_alert(keywords=detected, transcript=result['text'])
```

---

## üìä Sound Classification Classes

### Security-Critical Sounds (YAMNet)

| Class ID | Sound | Typical dB | Detection Confidence |
|----------|-------|------------|---------------------|
| 427 | Gunshot/Gunfire | 140-190 dB | 0.85+ |
| 401 | Explosion | 120-180 dB | 0.90+ |
| 376 | Glass Breaking | 100-120 dB | 0.80+ |
| 389 | Scream | 110-130 dB | 0.75+ |
| 423 | Alarm (Fire/Burglar) | 85-120 dB | 0.85+ |
| 312 | Siren (Police/Ambulance) | 110-130 dB | 0.90+ |
| 298 | Dog Barking (aggressive) | 85-110 dB | 0.70+ |

### Common Environmental Sounds

| Class ID | Sound | Use Case |
|----------|-------|----------|
| 0 | Speech/Conversation | Meeting detection |
| 137 | Music | Noise filtering |
| 188 | Door slam | Entry/exit monitoring |
| 245 | Footsteps | Presence detection |
| 367 | Car horn | Traffic monitoring |
| 412 | Baby crying | Daycare monitoring |

**Full list**: 521 AudioSet classes

---

## üöÄ Quick Start

### 1. **Simple Speech Transcription**

Drag nodes to canvas:
```
Camera ‚Üí Audio Extractor ‚Üí Whisper (Base) ‚Üí Debug Console
```

Configure:
- Audio Extractor: 16kHz, Mono, 5s buffer
- Whisper: Language = Auto Detect
- Click Execute

Watch Debug Console for transcriptions!

### 2. **Gunshot Detection System**

```
Security Camera ‚Üí Audio Extractor ‚Üí YAMNet ‚Üí Action (Emergency Alert)
```

Configure:
- Audio Extractor: 44.1kHz, Mono, 2s buffer
- YAMNet: Confidence = 0.85
- Action: Send immediate notification

### 3. **Keyword Alert System**

```
Reception Camera ‚Üí Audio Extractor ‚Üí Whisper (Tiny) ‚Üí Action (Conditional)
```

Configure:
- Whisper: Keywords = ["help", "emergency", "fire"]
- Action: If keywords detected ‚Üí Alert security team

---

## üí° Best Practices

### Performance Optimization

**1. Choose Right Model for Speed:**
```
Real-time alerts     ‚Üí Whisper Tiny + YAMNet
Transcription archive ‚Üí Whisper Small
Forensic analysis    ‚Üí Whisper Large
```

**2. Buffer Duration:**
```
Gunshot detection ‚Üí 2s (quick response)
Speech keywords   ‚Üí 5s (catch full phrases)
Meeting archive   ‚Üí 30s (reduce API calls)
```

**3. Sample Rate:**
```
Speech only      ‚Üí 16kHz (sufficient for Whisper)
Sound detection  ‚Üí 44.1kHz (capture full spectrum)
Bandwidth limited ‚Üí 8kHz (minimum for voice)
```

### Reducing False Positives

**For Sound Detection:**
- Increase confidence threshold (0.85+)
- Use multiple models for verification
- Add zone filtering (ignore audio from certain areas)
- Combine with video detection

**For Keywords:**
- Use longer buffer (catch full context)
- Lowercase matching
- Fuzzy matching for similar words
- Blacklist common false positives

---

## üîó Integration with Video Detection

### Combined Video + Audio Workflows

**Example: Comprehensive Security**
```
                    ‚îå‚Üí YOLOv8 (Person) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ                        ‚îÇ
Camera (RTSP) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚Üí Audio Extractor       ‚îú‚Üí Combine ‚Üí Alert
                    ‚îÇ      ‚Üì                 ‚îÇ
                    ‚îî‚Üí Whisper + YAMNet ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Trigger when:
- Person detected (video) AND
- Keyword spoken (audio) OR Gunshot sound (audio)
```

**Example: Abandoned Object Detection**
```
Video: Detect stationary object (bag, box)
Audio: Listen for ticking, beeping sounds
Combined: Alert on suspicious abandoned object
```

---

## üìã Supported Languages (Whisper)

Whisper supports 99 languages including:
- English, Spanish, French, German, Italian
- Portuguese, Russian, Japanese, Korean
- Chinese (Mandarin), Arabic, Hindi
- Dutch, Polish, Turkish, Vietnamese
- And 80+ more!

**Auto Detection** identifies language automatically.

---

## üéØ Common Sound Classes (YAMNet)

### Security
- Gunshot, Explosion, Glass breaking
- Scream, Crying, Shouting
- Alarm (fire, burglar), Siren
- Door slam, Window breaking

### Industrial
- Machine sounds, Motor, Engine
- Drill, Saw, Hammer
- Beep, Buzzer, Click
- Hum, Rattle, Squeak

### Environmental
- Dog bark, Cat meow
- Thunder, Wind, Rain
- Car horn, Traffic
- Footsteps, Door

### Voice
- Speech, Conversation
- Laughter, Cough, Sneeze
- Singing, Whistling

---

## üîß Installation (Backend)

### Install Dependencies

All required dependencies are included in `requirements.txt`:

```bash
pip install -r requirements.txt
```

**Audio Processing Dependencies:**
- `openai-whisper>=20231117` - Speech-to-text transcription
- `tensorflow>=2.13.0` - YAMNet sound classification
- `tensorflow-hub>=0.14.0` - Pre-trained audio models
- `librosa>=0.10.0` - Audio feature extraction
- `soundfile>=0.12.0` - Audio file I/O

### Download Models

```bash
# Whisper models auto-download on first use
# Or pre-download:
python -c "import whisper; whisper.load_model('base')"

# YAMNet model
python -c "import tensorflow_hub as hub; hub.load('https://tfhub.dev/google/yamnet/1')"
```

**Note:** Models are automatically downloaded on first use. Ensure sufficient disk space:
- Whisper Tiny: ~75 MB
- Whisper Base: ~140 MB
- Whisper Small: ~460 MB
- Whisper Medium: ~1.5 GB
- Whisper Large: ~2.9 GB
- YAMNet: ~50 MB

**GPU Support:**
Audio models will automatically use CUDA GPU if available for faster processing. CPU processing is fully supported for all models.

---

## üìä Performance Benchmarks

### Whisper Models (on CUDA GPU)

| Model | Speed (Real-time Factor) | VRAM | Use Case |
|-------|-------------------------|------|----------|
| Tiny | 32x (32s audio in 1s) | ~1GB | Real-time |
| Base | 16x | ~1GB | Live transcription |
| Small | 6x | ~2GB | Professional |
| Medium | 2x | ~5GB | High accuracy |
| Large | 1x | ~10GB | Best quality |

### Sound Classification (YAMNet)

- **Speed**: ~100x real-time (0.96s audio in 10ms)
- **Memory**: ~50MB
- **CPU**: Low (can run without GPU)

---

## üö® Security Use Cases

### 1. Active Shooter Detection
```
Workflow: Parking Lot + Building Cameras
Audio: YAMNet (gunshot detection, 0.90 threshold)
Video: Person detection + motion tracking
Action: Immediate lockdown + 911 call
```

### 2. Distress Call Detection
```
Workflow: All cameras with audio
Audio: Whisper Tiny (real-time)
Keywords: ["help", "emergency", "police", "fire"]
Action: Alert security desk with location
```

### 3. Perimeter Breach
```
Workflow: Fence-line cameras
Audio: YAMNet (glass breaking, metal sounds)
Video: Zone intrusion detection
Action: Security patrol dispatch
```

### 4. Vandalism Detection
```
Workflow: Building exterior
Audio: Glass breaking, spray paint hissing
Video: Person + suspicious objects
Action: Record + alert
```

---

## üè¢ Business Use Cases

### 5. Meeting Transcription
```
Conference Room Camera
‚Üí Audio Extractor (16kHz, 30s buffer)
‚Üí Whisper Small
‚Üí Store in database with searchable index
```

### 6. Customer Service QA
```
Service Desk Camera
‚Üí Audio: Whisper Base
‚Üí Keywords: ["complaint", "manager", "angry"]
‚Üí Flag conversations for review
```

### 7. Retail Analytics
```
Store Camera
‚Üí Audio: Count conversations
‚Üí Video: Count people
‚Üí Combined: Customer engagement metrics
```

---

## üìù API Endpoints

```
GET /api/workflow-components/audio-models
‚Üí Returns list of available audio AI models

POST /api/audio/transcribe
‚Üí Transcribe audio chunk with Whisper

POST /api/audio/classify
‚Üí Classify sounds with YAMNet/AST

GET /api/audio/sound-classes
‚Üí Get full list of detectable sounds
```

---

## üéä Example Workflows

### Security Monitoring (Full Spectrum)
```
Camera (High-security Zone)
  ‚îú‚Üí Video: YOLOv8 (person, weapon detection)
  ‚îî‚Üí Audio Extractor
      ‚îú‚Üí Whisper (keyword detection)
      ‚îî‚Üí YAMNet (gunshot, alarm)
         ‚Üì
     Combined Analysis
         ‚Üì
     Smart Alerting (reduce false positives)
```

### Meeting Room Intelligence
```
Conference Camera
  ‚îî‚Üí Audio Extractor
      ‚îî‚Üí Whisper Small
          ‚îú‚Üí Full Transcription ‚Üí Archive
          ‚îú‚Üí Keyword Search ‚Üí Action Items
          ‚îî‚Üí Speaker Count ‚Üí Attendance Tracking
```

### Industrial Safety Monitor
```
Factory Camera
  ‚îú‚Üí Video: PPE detection (hard hats, vests)
  ‚îî‚Üí Audio: YAMNet
      ‚îú‚Üí Alarm sounds ‚Üí Verify evacuation
      ‚îú‚Üí Machine sounds ‚Üí Anomaly detection
      ‚îî‚Üí Human speech ‚Üí Presence in restricted areas
```

---

## ‚úÖ Implementation Status

### ‚úÖ Frontend Complete
- Audio Extractor Node
- Audio AI Node
- Audio models endpoint
- Schema definitions
- Sidebar integration

### ‚úÖ Backend Implemented
- ‚úÖ Audio extraction from RTSP streams (using PyAV)
- ‚úÖ Whisper model integration (all variants: tiny, base, small, medium, large)
- ‚úÖ YAMNet sound classification integration
- ‚úÖ Keyword matching logic for transcriptions
- ‚úÖ Audio action handlers (events, alerts, webhooks)
- ‚úÖ Audio buffer management for chunked processing
- ‚úÖ Real-time audio processing in workflow executor

### Backend Architecture

**Audio Processing Pipeline:**
```
RTSPStream ‚Üí AudioExtractor ‚Üí AudioBuffer ‚Üí AudioAI (Whisper/YAMNet) ‚Üí EventBus ‚Üí Actions
```

**Key Components:**
1. **AudioExtractor** (`backend/stream/audio_extractor.py`) - Extracts audio from RTSP streams using PyAV
2. **AudioBuffer** (`backend/stream/audio_buffer.py`) - Thread-safe circular buffer for audio chunks
3. **WhisperModel** (`backend/models/whisper_model.py`) - Speech-to-text transcription
4. **YAMNetModel** (`backend/models/yamnet_model.py`) - Sound classification (521 classes)
5. **RealtimeWorkflowExecutor** - Orchestrates audio processing alongside video

**Model Registry:**
- Whisper variants: `whisper-tiny`, `whisper-base`, `whisper-small`, `whisper-medium`, `whisper-large`
- Sound classifiers: `yamnet`, `audio-spectrogram-transformer`, `panns-cnn14`

**Event System:**
- Audio events: `AUDIO_EXTRACTED`, `AUDIO_TRANSCRIBED`, `SOUND_DETECTED`, `KEYWORD_DETECTED`
- Integration with existing workflow event bus
- WebSocket broadcasting to frontend

---

## üéØ Quick Reference

### Node Connections
```
Video Stream ‚Üí Audio Extractor ‚Üí Audio AI ‚Üí Action/Debug
```

### Audio AI Settings

**Whisper (Transcription):**
- Model: tiny/base/small/medium/large
- Language: auto or specific
- Keywords: Optional trigger words
- Buffer: 5-30s recommended

**YAMNet (Sound Classification):**
- Confidence: 0.70-0.90
- Target sounds: Select from 521 classes
- Buffer: 2-5s recommended

---

*Last updated: October 30, 2025*  
*Version: 1.0.0*  
*Status: ‚úÖ Fully Implemented (Frontend + Backend)*

